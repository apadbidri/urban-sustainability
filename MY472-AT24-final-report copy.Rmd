---
title: "Urban Sustainability and Bike-Sharing: A Global Perspective"
author: "Candidate Number: 43182"
date: "[Insert the date of your final version here]"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Prompt:** 1

**ChatGPT/AI disclosure statement:** ChatGPT was used to help brainstorm ideas and clarify concepts. However, all data collection, analysis, and writing were carried out independently.

## 1. Introduction 

You should begin your report with a concise and engaging introduction. At a minimum, this introduction should describe the data you are collecting, why it is important and interesting, and what it could be used for. Your introduction should also narrow the scope of the prompt in a way that is both clear and engaging to the reader. For example, if you choose prompt 3, your introduction should make clear what organisation you work for and what objective it is struggling to meet.


## 2. Primary Data Collection

Pulling information about city bikes:

```{r}
# Load required libraries
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in
desired_networks <- c("Nilesplit", "LinkBike")  # Add more network names here

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% desired_networks) {
        network_id <- network$id
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add a new column for the network name
        stations <- network_details$network$stations
        stations$network_name <- network_name
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      
        print(citybike_info)
      }
    }
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}

```

You should explain what data you are collecting, how you are collecting it and why it can be considered your "primary" data.

You should explain whether there are any limits on the data you collected, and how you complied. You should also describe any ethical concerns or considerations in your data collection (e.g., when web scraping) and how you dealt with that in your code to ensure a responsible workflow.


## 3. Secondary Data Collection 

Information about pop density

``` {r}
# Load required libraries
library(rvest)
library(tibble)

# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("London", "New_York_City")

# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())

# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
  # Construct the full URL for each city
  url <- paste0(base_url, city)
  
  # Read the HTML content of the webpage
  webpage <- read_html(url)
  
  # Extract the population density value using the provided XPath
  pop_density <- webpage %>%
    html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]/tbody/tr[27]/td/text()[1]') %>%
    html_text() %>%
    trimws()  # Remove any leading or trailing whitespace
  
  # Add the city and its population density to the tibble
  city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}

# Print the final tibble
print(city_data)
```


## 4. Tabular data and transformations

[The text and code for this section goes here.]

## 5. Data Visualisation

[The text and code for this section goes here.]

## 6. Data output and storage 

[The text and code for this section goes here.]