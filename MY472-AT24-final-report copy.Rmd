---
title: "Urban Sustainability and Bike-Sharing: A Global Perspective"
author: "Candidate Number: 43182"
date: "[Insert the date of your final version here]"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Prompt:** 1

**ChatGPT/AI disclosure statement:** ChatGPT was used to help brainstorm ideas and clarify concepts. However, all data collection, analysis, and writing were carried out independently.

## 1. Introduction 

You should begin your report with a concise and engaging introduction. At a minimum, this introduction should describe the data you are collecting, why it is important and interesting, and what it could be used for. Your introduction should also narrow the scope of the prompt in a way that is both clear and engaging to the reader. For example, if you choose prompt 3, your introduction should make clear what organisation you work for and what objective it is struggling to meet.


## 2. Primary Data Collection

Super urban cities

```{r}
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in and their corresponding cities
network_cities <- c(
  "HELLO CYCLING Tokyo" = "Tokyo", 
  "Vélib' Métropole" = "Paris", 
  "Gira" = "Lisbon", 
  "Citi Bike" = "New York City", 
  "BikeSampa" = "São Paulo", 
  "Dubai Careem BIKE" = "Dubai", 
  "Bicing" = "Barcelona", 
  "Seoul Bike 따릉이" = "Seoul", 
  "Tembici" = "Bogotá", 
  "Beryl - Greater Manchester" = "Manchester"
)

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% names(network_cities)) {
        network_id <- network$id
        city_name <- network_cities[network_name]  # Get the corresponding city name
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add new columns for the network name and city
        stations <- network_details$network$stations
        stations$network_name <- network_name
        stations$city_name <- city_name  # Add the city name as a new column
        
        # Ensure that the 'extra$uid' column is of the same type across all networks
        if("extra" %in% names(stations)) {
          stations$extra$uid <- as.character(stations$extra$uid)
        }
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      }
    }
    
    # Print the combined citybike_info data with the new city_name column
    print(citybike_info)
    
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}
```

Pulling bike info for the rural places 
```{r}
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in
desired_networks <- c("Slovnaft BAjk", "Medina Careem BIKE" )  # Add more network names here

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% desired_networks) {
        network_id <- network$id
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add a new column for the network name
        stations <- network_details$network$stations
        stations$network_name <- network_name
        
        # Ensure that the 'extra$uid' column is of the same type across all networks
        if("extra" %in% names(stations)) {
          stations$extra$uid <- as.character(stations$extra$uid)
        }
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      }
    }
    
    # Print the combined citybike_info data
    print(citybike_info)
    
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}

```
```




You should explain what data you are collecting, how you are collecting it and why it can be considered your "primary" data.

You should explain whether there are any limits on the data you collected, and how you complied. You should also describe any ethical concerns or considerations in your data collection (e.g., when web scraping) and how you dealt with that in your code to ensure a responsible workflow.


## 3. Secondary Data Collection 

Information about pop density

``` {r}
library(rvest)
library(tibble)

# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Penang")

# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())

# Define the CSS selector for population density
pop_density_selector <- "#mw-content-text > div.mw-content-ltr.mw-parser-output > table.infobox.ib-pol-div.vcard > tbody > tr:nth-child(32)"
area_selector <- "#mw-content-text > div.mw-content-ltr.mw-parser-output > table.infobox.ib-pol-div.vcard > tbody > tr:nth-child(32)"


# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
  # Construct the full URL for each city
  url <- paste0(base_url, city)
  
  # Read the HTML content of the webpage
  webpage <- read_html(url)
  
  # Extract the population density value using the provided CSS selector
  pop_density <- webpage %>%
    html_nodes(css = pop_density_selector) %>%
    html_text() %>% 
    trimws()  # Remove any leading or trailing whitespace
  
   area <- webpage %>%
    html_nodes(css = area_selector) %>%
    html_text() %>%
    trimws() 
  
  # If the population density is not found, set it as NA
  pop_density <- ifelse(length(pop_density) == 0 || pop_density == "", NA, pop_density)
  area <- ifelse(length(area) == 0 || area == "", NA, area)
  
  # Add the city and its population density to the tibble
  city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density, Area = area))
}

# Print the final tibble
print(city_data)

```
```{r}
# Load required libraries
library(httr)
library(jsonlite)
library(tibble)

# Your OpenAQ API Key
api_key <- "5a43ffdd3f5fbba9f309a5114f194a9760851ed941b45a9e999ecd82c73616a1"

# Base API URL for measurements
url <- "https://explore.openaq.org/locations/3316776"

# Query parameters for the API request
params <- list(
  location_id = 2884480,  # Location ID for your desired location
  parameter = "pm25",     # Pollutant parameter
  limit = 100,            # Maximum number of results to retrieve
  order_by = "datetime"   # Order results by date
)

# Send GET request with API key in the header
response <- GET(
  url = url,
  query = params,
  add_headers(`X-API-Key` = api_key)
)

# Check response status
if (http_status(response)$category == "Success") {
  # Parse JSON content
  data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
  
  # Convert the results into a tibble
  if (!is.null(data$results) && length(data$results) > 0) {
    pm25_data <- as_tibble(data$results)
    print(pm25_data)
  } else {
    cat("No PM2.5 data available for this location.\n")
  }
} else {
  # Handle errors
  cat("Error:", http_status(response)$message, "\n")
}

```

## 4. Tabular data and transformations

[The text and code for this section goes here.]

## 5. Data Visualisation

[The text and code for this section goes here.]

## 6. Data output and storage 

[The text and code for this section goes here.]