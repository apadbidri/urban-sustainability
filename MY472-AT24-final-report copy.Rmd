---
title: "Urban Sustainability and Bike-Sharing: A Global Perspective"
author: "Candidate Number: 43182"
date: "[Insert the date of your final version here]"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Prompt:** 1

**ChatGPT/AI disclosure statement:** ChatGPT was used to help brainstorm ideas and clarify concepts. However, all data collection, analysis, and writing were carried out independently.

## 1. Introduction 

You should begin your report with a concise and engaging introduction. At a minimum, this introduction should describe the data you are collecting, why it is important and interesting, and what it could be used for. Your introduction should also narrow the scope of the prompt in a way that is both clear and engaging to the reader. For example, if you choose prompt 3, your introduction should make clear what organisation you work for and what objective it is struggling to meet.


## 2. Primary Data Collection

Super urban cities

```{r}
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in and their corresponding cities
network_cities <- c(
  "HELLO CYCLING Tokyo" = "Tokyo", 
  "Vélib' Métropole" = "Paris", 
  "Gira" = "Lisbon", 
  "Citi Bike" = "New York City", 
  "BikeSampa" = "São Paulo", 
  "Dubai Careem BIKE" = "Dubai", 
  "Bicing" = "Barcelona", 
  "Seoul Bike 따릉이" = "Seoul", 
  "Tembici" = "Bogotá", 
  "Beryl - Greater Manchester" = "Manchester",
  "Xi'an Public Bicycle" = "Xi'An", 
  "Medina Careem BIKE" = "Medina", 
  "LinkBike" = "Penang", 
  "ThessBike" = "Thessaloniki", 
  "BisPArks BCycle" = "North Dakota", 
  "Accès Vélo" = "Saguenay", 
  "Slovnaft BAjk" = "Bratislava", 
  "SiXT" = "Riga", 
  "Skrova Bysykkel" = "Skrova", 
  "Shymkentbike" = "Shymkent"
)

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% names(network_cities)) {
        network_id <- network$id
        city_name <- network_cities[network_name]  # Get the corresponding city name
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add new columns for the network name and city
        stations <- network_details$network$stations
        stations$network_name <- network_name
        stations$city_name <- city_name  # Add the city name as a new column
        
        # Ensure that the 'extra$uid' column is of the same type across all networks
        if("extra" %in% names(stations)) {
          stations$extra$uid <- as.character(stations$extra$uid)
        }
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      }
    }
    
    # Print the combined citybike_info data with the new city_name column
    print(citybike_info)
    
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}
```

```{r}
library(httr)
library(jsonlite)
library(tibble)
library(purrr)
library(dplyr)

# Define cities with names and coordinates 
# ChatGPT was used to form this code chunk below pertaining to the list of cities with their corresponding coordinates 
cities <- tibble::tibble(
  city_name = c(
    "Tokyo", "Paris", "Lisbon", "New York City", "São Paulo",
    "Dubai", "Barcelona", "Seoul", "Bogotá", "Manchester",
    "Xi'An", "Medina", "Penang", "Thessaloniki", "North Dakota",
    "Saguenay", "Bratislava", "Riga", "Skrova", "Shymkent"
  ),
  Latitude = c(
    35.6895, 48.8566, 38.7169, 40.7128, -23.5505,
    25.2769, 41.3851, 37.5665, 4.711, 53.4808,
    34.3416, 24.4667, 5.3729, 40.6401, 47.1164,
    48.4284, 48.1486, 56.9496, 68.1178, 42.3174
  ),
  Longitude = c(
    139.6917, 2.3522, -9.139, -74.006, -46.6333,
    55.297, 2.1734, 126.978, -74.0721, -2.2426,
    108.9398, 39.5994, 100.2496, 22.9444, -101.2996,
    -71.0657, 17.1077, 24.1052, 14.5146, 69.5914
  )
)

# Function to fetch AQI for a single city
get_aqi <- function(latitude, longitude) {
  # Construct the API URL
  api_url <- sprintf(
    "https://air-quality-api.open-meteo.com/v1/air-quality?latitude=%f&longitude=%f&current=us_aqi",
    latitude, longitude
  )
  
  # Make the API request
  response <- GET(api_url)
  
  # Parse the response and extract AQI
  if (http_type(response) == "application/json") {
    air_quality_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
    return(air_quality_data$current$us_aqi)
  } else {
    return(NA)  # Return NA if API call fails
  }
}

# Apply the function to all cities and add the AQI to the tibble
city_aqi_data <- cities %>%
  mutate(AQI = map2_dbl(Latitude, Longitude, get_aqi))

city_aqi_data <- city_aqi_data %>%
  select(city_name, AQI)

# Print the final tibble
print(city_aqi_data)
```
Getting info about square 

```{r}

library(rvest)
library(tibble)

# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Paris", "Lisbon", "New_York_City", "São_Paulo", "Dubai", "Barcelona", "Seoul", 
            "Bogotá", "Manchester", "Xi%27an", "Medina", "Penang", "Thessaloniki", "North_Dakota", 
           "Bratislava", "Riga", "Skrova", "Shymkent")

# Define a named vector for cities and their corresponding XPath selectors for population density
city_xpaths <- c(
  "Tokyo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[22]/td/text()[1]",
  "Paris" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td/text()[1]",
  "Lisbon" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()[1]",
  "New_York_City" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[20]/td/text()[1]",
  "São_Paulo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[24]/td/text()[1]",
  "Dubai" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[14]/td/text()[1]",
  "Barcelona" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td/text()[1]",
  "Seoul" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[26]/td/text()[1]",
  "Bogotá" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[18]/td/text()[1]",
  "Manchester" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[27]/td/text()[1]",
  "Xi%27an" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[20]/td/text()[1]",
  "Medina" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[20]/td/text()[1]",
  "Penang" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[32]/td/text()[1]", 
  "Thessaloniki" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[21]/td/text()[1]",
  "North_Dakota" = "//*[@id='mw-content-text']/div[1]/table[2]/tbody/tr[25]/td/text()[1]", 
  "Bratislava" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td/text()[1]",
  "Riga" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[14]/td/text()[1]", 
  "Skrova" = "//*[@id='mw-content-text']/div[1]/table/tbody/tr[9]/td", 
  "Shymkent" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[14]/td/text()[1]"
    
  # Add more cities and their corresponding XPaths here as needed
)

# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
area_data <- tibble(city_name = character(), area = character())

# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
  # Construct the full URL for each city
  url <- paste0(base_url, city)
  
  # Read the HTML content of the webpage
  webpage <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error reading ", city, " page: ", e$message)
    return(NULL)
  })
  
  if (!is.null(webpage)) {
    # Extract the population density value using the corresponding XPath for each city
    area_xpath <- city_xpaths[city]  # Get the correct XPath for the city
    area <- webpage %>%
      html_nodes(xpath = area_xpath) %>%
      html_text() %>% 
      trimws()  # Remove any leading or trailing whitespace
    
    # If the population density is not found, set it as NA
    if (length(area) > 0) {
      area <- area[1]  # Take the first value if there are multiple
    } else {
      area <- NA
    }
    
    # Add the city and its population density to the tibble
    area_data <- bind_rows(area_data, tibble(city_name = city, area = area))
  }
}


# Replace underscores with spaces in city names
area_data$city_name <- gsub("_", " ", area_data$city_name)

# Print the final tibble
print(area_data)
```
# Merging into one dataset

```{r}

# Merge the citybike_info with city_aqi_data based on the City column
merged_data <- left_join(citybike_info, city_aqi_data, by = "city_name")

# Rename 
almost_data <- merged_data

# Print the final merged data
print(almost_data)

```
```{r}

# Merge the citybike_info with city_aqi_data based on the City column
primary_data <- left_join(almost_data, area_data, by = "city_name")

# Print the final merged data
print(primary_data)

```



Cleaning up primary_data

```{r}
library(tidyr)
library(dplyr)

# Assuming 'primary_data' contains the long-format data

primary_data_wide <- primary_data %>%
  group_by(city_name) %>%
  summarise(
    bike_stations = n(),  # Number of bike stations per city
    free_bikes = sum(free_bikes, na.rm = TRUE),  # Sum of free bikes per city
    network_name = unique(network_name),  # Assuming each city has one network
    AQI = unique(AQI),
    area = unique(area) # Assuming each city has one AQI value
  ) %>%
  ungroup()


# Remove any non-numeric characters (like commas, text)
primary_data_wide$area <- gsub(",", "", primary_data_wide$area)  # Remove commas
primary_data_wide$area <- gsub("[^0-9.]", "", primary_data_wide$area)  # Remove any non-numeric characters

# Convert the 'area' column to numeric first
primary_data_wide$area <- as.numeric(primary_data_wide$area)

# Now convert to integer
primary_data_wide$area <- as.integer(primary_data_wide$area)

# Print the updated dataset
print(primary_data_wide)
```

## 3. Secondary Data Collection 

Information about population

```{r}
library(rvest)
library(tibble)
library(dplyr)

# Define the list of city names
cities <- c("Tokyo", "Paris", "Lisbon", "New_York_City", "São_Paulo", "Dubai", 
            "Barcelona", "Seoul", "Bogotá", "Manchester", "Xi%27an", "Medina", 
            "Penang", "Nicosia", "North_Dakota", "Bratislava", "Riga", 
            "Rieti", "Lima", "Inverness")

# Define a named vector for cities and their corresponding XPath selectors for population and population density
city_xpaths <- list(
  population = c(
    "Tokyo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[28]/td",
    "Paris" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[21]/td",
    "Lisbon" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[29]/td/text()",
    "New_York_City" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[26]/td",
    "São_Paulo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[29]/td",
    "Dubai" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[16]/td",
    "Barcelona" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[22]/td",
    "Seoul" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[31]/td",
    "Bogotá" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[23]/td/text()",
    "Manchester" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td",
    "Xi%27an" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[26]/td",
    "Medina" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td",
    "Penang" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[36]/td", 
    "Nicosia" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td",
    "North_Dakota" = "//*[@id='mw-content-text']/div[1]/table[2]/tbody/tr[35]/td/text()", 
    "Bratislava" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()",
    "Riga" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[18]/td", 
    "Rieti" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td", 
    "Lima" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td",
    "Inverness" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[24]/td"
  ),
  pop_density = c(
    "Tokyo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()[1]",
    "Paris" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[23]/td/text()[1]",
    "Lisbon" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()[1]",
    "New_York_City" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[29]/td/text()[1]",
    "São_Paulo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[31]/td/text()[1]",
    "Dubai" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[17]/td/text()[1]",
    "Barcelona" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[24]/td/text()[1]",
    "Seoul" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[33]/td/text()[1]",
    "Bogotá" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()[1]",
    "Manchester" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[32]/td/text()[1]",
    "Xi%27an" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[27]/td/text()[1]",
    "Medina" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[27]/td/text()[1]",
    "Penang" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[37]/td/text()[1]", 
    "Nicosia" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[26]/td/text()[1]",
    "North_Dakota" = "//*[@id='mw-content-text']/div[1]/table[2]/tbody/tr[37]/td/text()[1]", 
    "Bratislava" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[28]/td/text()[1]",
    "Riga" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td/text()[1]", 
    "Rieti" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[20]/td/text()[1]", 
    "Lima" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[26]/td/text()[1]",
    "Inverness" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()[1]"
  )
)

# Base URL for Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
city_info <- tibble(city_names = character(), population = character(), pop_density = character())

# Loop through each city
for (city in cities) {
  # Construct the full URL
  url <- paste0(base_url, city)
  
  # Try reading the webpage
  webpage <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error reading ", city, " page: ", e$message)
    return(NULL)
  })
  
  if (!is.null(webpage)) {
    # Extract population and population density
    population <- webpage %>%
      html_nodes(xpath = city_xpaths$population[city]) %>%
      html_text() %>%
      trimws()
    if (length(population) == 0) population <- NA
    
    pop_density <- webpage %>%
      html_nodes(xpath = city_xpaths$pop_density[city]) %>%
      html_text() %>%
      trimws()
    if (length(pop_density) == 0) pop_density <- NA
    
    # Add results to tibble
    city_info <- bind_rows(city_info, tibble(
      city_names = city,
      population = population,
      pop_density = pop_density
    ))
  }
}

secondary_data <- city_info

# Print final tibble
print(secondary_data)
```


## 4. Tabular data and transformations

Air Quality Denomination 
```{r}
library(dplyr)


# Assuming primary_data is your dataframe that contains the AQI column
primary_data_wide <- primary_data_wide %>%
  mutate(
    air_quality = case_when(
      AQI <= 50 ~ "Good",
      AQI > 50 & AQI <= 100 ~ "Moderate",
      AQI > 100 & AQI <= 150 ~ "Unhealthy for sensitive groups",
      AQI > 150 & AQI <= 200 ~ "Unhealthy",
      AQI > 200 & AQI <= 300 ~ "Very unhealthy",
      AQI > 300 ~ "Hazardous",
      TRUE ~ "Unknown"  # Handle any unexpected AQI values
    )
  )

# View the updated dataframe with the new air_quality column
print(primary_data_wide)

library(dplyr)
```

Population Density Category 
```{r}
library(dplyr)

secondary_data <- secondary_data %>%
  mutate(
    pop_density_class = case_when(
      pop_density <= 1000 ~ "Low",
      pop_density > 1000 & pop_density <= 5000 ~ "Medium",
      pop_density > 5000 & pop_density <= 15000 ~ "High",
      pop_density > 15000 ~ "Very High",
      TRUE ~ NA_character_  # In case there are any NA or unexpected values
    )
  )

# View the updated data
print(secondary_data)
```

Ratio of bike stations per square km
```{r}
# Convert area from square kilometers to square miles
primary_data_wide$area_miles <- primary_data_wide$area * 0.3861

# Now calculate stations per square mile
primary_data_wide$stations_per_sqmi <- round(primary_data_wide$bike_stations / primary_data_wide$area_miles)

print(primary_data_wide)
```

Bike availability category
```{r}
# Classify cities based on bike station availability
high_threshold <- median(primary_data_wide$bike_stations, na.rm = TRUE)
primary_data_wide$bike_station_category <- ifelse(primary_data_wide$bike_stations > high_threshold, 
                                                   "High Availability", 
                                                   "Low Availability")

# Print the updated dataset
print(primary_data_wide)
```
AQI and Bike Stations Interaction Index
High index - could suggest that cities with high number of bike stations have lower AQI
Low index - could suggest that cities with low number of bike stations have higher AQI 

```{r}

# Assuming your dataset is named `df1`
# Columns in df1: 'city', 'AQI', 'bike_station_density'

# Step 1: Create the Interaction Index
primary_data_wide$interaction_index <- round(primary_data_wide$bike_stations / primary_data_wide$AQI)

# Step 2: Handle potential division-by-zero or NA issues
primary_data_wide$interaction_index[is.infinite(primary_data_wide$interaction_index)] <- NA  # Replace Inf with NA
primary_data_wide$interaction_index[is.na(primary_data_wide$interaction_index)] <- 0        # Replace NA with 0

# Step 3: View the updated dataset
print(primary_data_wide)

```



## 5. Data Visualisation

[The text and code for this section goes here.]

## 6. Data output and storage 

[The text and code for this section goes here.]