---
title: "Urban Sustainability and Bike-Sharing: A Global Perspective"
author: "Candidate Number: 43182"
date: "[Insert the date of your final version here]"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Prompt:** 1

**ChatGPT/AI disclosure statement:** ChatGPT was used to help brainstorm ideas and clarify concepts. However, all data collection, analysis, and writing were carried out independently.

## 1. Introduction 

You should begin your report with a concise and engaging introduction. At a minimum, this introduction should describe the data you are collecting, why it is important and interesting, and what it could be used for. Your introduction should also narrow the scope of the prompt in a way that is both clear and engaging to the reader. For example, if you choose prompt 3, your introduction should make clear what organisation you work for and what objective it is struggling to meet.


## 2. Primary Data Collection

Super urban cities

```{r}
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in and their corresponding cities
network_cities <- c(
  "HELLO CYCLING Tokyo" = "Tokyo", 
  "Vélib' Métropole" = "Paris", 
  "Gira" = "Lisbon", 
  "Citi Bike" = "New York City", 
  "BikeSampa" = "São Paulo", 
  "Dubai Careem BIKE" = "Dubai", 
  "Bicing" = "Barcelona", 
  "Seoul Bike 따릉이" = "Seoul", 
  "Tembici" = "Bogotá", 
  "Beryl - Greater Manchester" = "Manchester",
  "Xi'an Public Bicycle" = "Xi'An", 
  "Medina Careem BIKE" = "Medina", 
  "LinkBike" = "Penang", 
  "ThessBike" = "Thessaloniki", 
  "BisPArks BCycle" = "North Dakota", 
  "Accès Vélo" = "Saguenay", 
  "Slovnaft BAjk" = "Bratislava", 
  "SiXT" = "Riga", 
  "Skrova Bysykkel" = "Skrova", 
  "Shymkentbike" = "Shymkent"
)

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% names(network_cities)) {
        network_id <- network$id
        city_name <- network_cities[network_name]  # Get the corresponding city name
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add new columns for the network name and city
        stations <- network_details$network$stations
        stations$network_name <- network_name
        stations$city_name <- city_name  # Add the city name as a new column
        
        # Ensure that the 'extra$uid' column is of the same type across all networks
        if("extra" %in% names(stations)) {
          stations$extra$uid <- as.character(stations$extra$uid)
        }
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      }
    }
    
    # Print the combined citybike_info data with the new city_name column
    print(citybike_info)
    
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}
```

```{r}
library(httr)
library(jsonlite)
library(tibble)
library(purrr)
library(dplyr)

# Define cities with names and coordinates 
# ChatGPT was used to form this code chunk below pertaining to the list of cities with their corresponding coordinates 
cities <- tibble::tibble(
  city_name = c(
    "Tokyo", "Paris", "Lisbon", "New York City", "São Paulo",
    "Dubai", "Barcelona", "Seoul", "Bogotá", "Manchester",
    "Xi'An", "Medina", "Penang", "Thessaloniki", "North Dakota",
    "Saguenay", "Bratislava", "Riga", "Skrova", "Shymkent"
  ),
  Latitude = c(
    35.6895, 48.8566, 38.7169, 40.7128, -23.5505,
    25.2769, 41.3851, 37.5665, 4.711, 53.4808,
    34.3416, 24.4667, 5.3729, 40.6401, 47.1164,
    48.4284, 48.1486, 56.9496, 68.1178, 42.3174
  ),
  Longitude = c(
    139.6917, 2.3522, -9.139, -74.006, -46.6333,
    55.297, 2.1734, 126.978, -74.0721, -2.2426,
    108.9398, 39.5994, 100.2496, 22.9444, -101.2996,
    -71.0657, 17.1077, 24.1052, 14.5146, 69.5914
  )
)

# Function to fetch AQI for a single city
get_aqi <- function(latitude, longitude) {
  # Construct the API URL
  api_url <- sprintf(
    "https://air-quality-api.open-meteo.com/v1/air-quality?latitude=%f&longitude=%f&current=us_aqi",
    latitude, longitude
  )
  
  # Make the API request
  response <- GET(api_url)
  
  # Parse the response and extract AQI
  if (http_type(response) == "application/json") {
    air_quality_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
    return(air_quality_data$current$us_aqi)
  } else {
    return(NA)  # Return NA if API call fails
  }
}

# Apply the function to all cities and add the AQI to the tibble
city_aqi_data <- cities %>%
  mutate(AQI = map2_dbl(Latitude, Longitude, get_aqi))

city_aqi_data <- city_aqi_data %>%
  select(city_name, AQI)

# Print the final tibble
print(city_aqi_data)
```
# Merging into one dataset

```{r}

# Merge the citybike_info with city_aqi_data based on the City column
merged_data <- left_join(citybike_info, city_aqi_data, by = "city_name")

# Rename 
primary_data <- merged_data

# Print the final merged data
print(primary_data)

```


## 3. Secondary Data Collection 

Information about pop density

```{r}
library(rvest)
library(tibble)

# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Paris", "Lisbon", "New_York_City", "São_Paulo", "Dubai", "Barcelona", "Seoul", 
            "Bogotá", "Manchester", "Xi%27an", "Medina", "Penang", "Thessaloniki", "North_Dakota", 
           "Bratislava", "Riga", "Skrova", "Shymkent")

# Define a named vector for cities and their corresponding XPath selectors for population density
city_xpaths <- c(
  "Tokyo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()[1]",
  "Paris" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[23]/td/text()[1]",
  "Lisbon" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()[1]",
  "New_York_City" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[29]/td/text()[1]",
  "São_Paulo" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[31]/td/text()[1]",
  "Dubai" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[17]/td/text()[1]",
  "Barcelona" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[24]/td/text()[1]",
  "Seoul" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[33]/td/text()[1]",
  "Bogotá" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()[1]",
  "Manchester" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[32]/td/text()[1]",
  "Xi%27an" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[25]/td/text()[1]",
  "Medina" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[27]/td/text()[1]",
  "Penang" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[37]/td/text()[1]", 
  "Thessaloniki" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[28]/td/text()",
  "North_Dakota" = "//*[@id='mw-content-text']/div[1]/table[2]/tbody/tr[37]/td/text()[1]", 
  "Bratislava" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[28]/td/text()[1]",
  "Riga" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[19]/td/text()[1]", 
  "Skrova" = "//*[@id='mw-content-text']/div[1]/table/tbody/tr[7]/td", 
  "Shymkent" = "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[17]/td/text()"
    
  # Add more cities and their corresponding XPaths here as needed
)

# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())

# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
  # Construct the full URL for each city
  url <- paste0(base_url, city)
  
  # Read the HTML content of the webpage
  webpage <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error reading ", city, " page: ", e$message)
    return(NULL)
  })
  
  if (!is.null(webpage)) {
    # Extract the population density value using the corresponding XPath for each city
    pop_density_xpath <- city_xpaths[city]  # Get the correct XPath for the city
    pop_density <- webpage %>%
      html_nodes(xpath = pop_density_xpath) %>%
      html_text() %>% 
      trimws()  # Remove any leading or trailing whitespace
    
    # If the population density is not found, set it as NA
    if (length(pop_density) > 0) {
      pop_density <- pop_density[1]  # Take the first value if there are multiple
    } else {
      pop_density <- NA
    }
    
    # Add the city and its population density to the tibble
    city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
  }
}

secondary_data <- city_data 

# Print the final tibble
print(secondary_data)
```

## 4. Tabular data and transformations

Air Quality Denomination 
```{r}
library(dplyr)


# Assuming primary_data is your dataframe that contains the AQI column
primary_data <- primary_data %>%
  mutate(
    air_quality = case_when(
      AQI <= 50 ~ "Good",
      AQI > 50 & AQI <= 100 ~ "Moderate",
      AQI > 100 & AQI <= 150 ~ "Unhealthy for sensitive groups",
      AQI > 150 & AQI <= 200 ~ "Unhealthy",
      AQI > 200 & AQI <= 300 ~ "Very unhealthy",
      AQI > 300 ~ "Hazardous",
      TRUE ~ "Unknown"  # Handle any unexpected AQI values
    )
  )

# View the updated dataframe with the new air_quality column
print(primary_data)

library(dplyr)
```

Population Densty Category 

```{r}
library(dplyr)

secondary_data <- secondary_data %>%
  mutate(
    pop_density_class = case_when(
      Population_Density <= 1000 ~ "Low",
      Population_Density > 1000 & Population_Density <= 5000 ~ "Medium",
      Population_Density > 5000 & Population_Density <= 15000 ~ "High",
      Population_Density > 15000 ~ "Very High",
      TRUE ~ NA_character_  # In case there are any NA or unexpected values
    )
  )

# View the updated data
print(secondary_data)
```

AQI Quartiles 

```{r}
```

AQI Quartiles 

```{r}
```

AQI Quartiles 

```{r}
```




## 5. Data Visualisation

[The text and code for this section goes here.]

## 6. Data output and storage 

[The text and code for this section goes here.]