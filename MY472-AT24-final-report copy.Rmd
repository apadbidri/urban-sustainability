---
title: "Urban Sustainability and Bike-Sharing: A Global Perspective"
author: "Candidate Number: 43182"
date: "[Insert the date of your final version here]"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Prompt:** 1

**ChatGPT/AI disclosure statement:** ChatGPT was used to help brainstorm ideas and clarify concepts. However, all data collection, analysis, and writing were carried out independently.

## 1. Introduction 

You should begin your report with a concise and engaging introduction. At a minimum, this introduction should describe the data you are collecting, why it is important and interesting, and what it could be used for. Your introduction should also narrow the scope of the prompt in a way that is both clear and engaging to the reader. For example, if you choose prompt 3, your introduction should make clear what organisation you work for and what objective it is struggling to meet.


## 2. Primary Data Collection

Super urban cities

```{r}
library(httr)
library(jsonlite)
library(dplyr)

# Base API URL
base_url <- "http://api.citybik.es/v2/networks"

# Initialize an empty data frame to store the results
citybike_info <- data.frame()

# List of network names you are interested in and their corresponding cities
network_cities <- c(
  "HELLO CYCLING Tokyo" = "Tokyo", 
  "Vélib' Métropole" = "Paris", 
  "Gira" = "Lisbon", 
  "Citi Bike" = "New York City", 
  "BikeSampa" = "São Paulo", 
  "Dubai Careem BIKE" = "Dubai", 
  "Bicing" = "Barcelona", 
  "Seoul Bike 따릉이" = "Seoul", 
  "Tembici" = "Bogotá", 
  "Beryl - Greater Manchester" = "Manchester",
  "Xi'an Public Bicycle" = "Xi'An", 
  "Medina Careem BIKE" = "Medina", 
  "LinkBike" = "Penang", 
  "ThessBike" = "Thessaloniki", 
  "BisPArks BCycle" = "North Dakota", 
  "Accès Vélo" = "Saguenay", 
  "Slovnaft BAjk" = "Bratislava", 
  "SiXT" = "Riga", 
  "Skrova Bysykkel" = "Skrova", 
  "Shymkentbike" = "Shymkent"
)

# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)

# Check if the response is in JSON format
if (http_type(response) == "application/json") {
  networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
  
  # Make sure the networks_data is a list and contains the 'networks' key
  if ("networks" %in% names(networks_data)) {
    networks_list <- networks_data$networks
    
    # Loop through each network and fetch its detailed information if it's in the desired list
    for (network in networks_list) {
      network_name <- network$name
      
      # Check if the network is in the list of desired networks
      if (network_name %in% names(network_cities)) {
        network_id <- network$id
        city_name <- network_cities[network_name]  # Get the corresponding city name
        
        # Construct the URL for the specific network's detailed information
        network_url <- paste0(base_url, "/", network_id)
        
        # Fetch detailed information about the network
        network_response <- GET(network_url)
        network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
        
        # Extract station details and add new columns for the network name and city
        stations <- network_details$network$stations
        stations$network_name <- network_name
        stations$city_name <- city_name  # Add the city name as a new column
        
        # Ensure that the 'extra$uid' column is of the same type across all networks
        if("extra" %in% names(stations)) {
          stations$extra$uid <- as.character(stations$extra$uid)
        }
        
        # Combine the new stations data with the citybike_info dataset
        citybike_info <- bind_rows(citybike_info, stations)
      }
    }
    
    # Print the combined citybike_info data with the new city_name column
    print(citybike_info)
    
  } else {
    cat("Error: 'networks' not found in the API response.\n")
  }
} else {
  cat("Failed to retrieve data from the CityBikes API.\n")
}
```


## 3. Secondary Data Collection 

Information about pop density

```{r}
library(rvest)
library(tibble)

# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Paris", "Lisbon", "New_York_City", "São_Paulo", "Dubai", "Barcelona", "Seoul", 
            "Bogotá", "Manchester", "Xi%27an", "Medina", "Penang", "Thessaloniki", "North_Dakota", 
            "Saguenay", "Bratislava", "Riga", "Skrova", "Shymkent")

# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"

# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())

# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"

# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
  # Construct the full URL for each city
  url <- paste0(base_url, city)
  
  # Read the HTML content of the webpage
  webpage <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error reading ", city, " page: ", e$message)
    return(NULL)
  })
  
  if (!is.null(webpage)) {
    # Extract the population density value using the provided XPath
    pop_density <- webpage %>%
      html_nodes(xpath = pop_density_xpath) %>%
      html_text() %>% 
      trimws()  # Remove any leading or trailing whitespace
    
    # If the population density is not found, set it as NA
    if (length(pop_density) > 0) {
      pop_density <- pop_density[1]  # Take the first value if there are multiple
    } else {
      pop_density <- NA
    }
    
    # Add the city and its population density to the tibble
    city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
  }
}

# Print the final tibble
print(city_data)
```



## 4. Tabular data and transformations

[The text and code for this section goes here.]

## 5. Data Visualisation

[The text and code for this section goes here.]

## 6. Data output and storage 

[The text and code for this section goes here.]