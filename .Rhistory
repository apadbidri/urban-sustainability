# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Fetch all networks to find LinkBike
response <- GET(base_url)
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
# Filter for LinkBike network
linkbike_network <- networks_data$networks[networks_data$networks$name == "ThessBike",]
if (nrow(linkbike_network) > 0) {
linkbike_id <- linkbike_network$id
linkbike_url <- paste0(base_url, "/", linkbike_id)
# Fetch detailed information about LinkBike network
linkbike_response <- GET(linkbike_url)
linkbike_data <- content(linkbike_response, as = "parsed", simplifyDataFrame = TRUE)
# Print detailed information
print(linkbike_data)
} else {
cat("LinkBike network not found in the API data.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
# Load required libraries
library(httr)
library(jsonlite)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Fetch all networks to find LinkBike
response <- GET(base_url)
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
# Filter for LinkBike network
linkbike_network <- networks_data$networks[networks_data$networks$name == "Chartered Bike",]
if (nrow(linkbike_network) > 0) {
linkbike_id <- linkbike_network$id
linkbike_url <- paste0(base_url, "/", linkbike_id)
# Fetch detailed information about LinkBike network
linkbike_response <- GET(linkbike_url)
linkbike_data <- content(linkbike_response, as = "parsed", simplifyDataFrame = TRUE)
# Print detailed information
print(linkbike_data)
} else {
cat("LinkBike network not found in the API data.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
# Load required libraries
library(httr)
library(jsonlite)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Fetch all networks to find LinkBike
response <- GET(base_url)
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
# Filter for LinkBike network
linkbike_network <- networks_data$networks[networks_data$networks$name == "Gira",]
if (nrow(linkbike_network) > 0) {
linkbike_id <- linkbike_network$id
linkbike_url <- paste0(base_url, "/", linkbike_id)
# Fetch detailed information about LinkBike network
linkbike_response <- GET(linkbike_url)
linkbike_data <- content(linkbike_response, as = "parsed", simplifyDataFrame = TRUE)
# Print detailed information
print(linkbike_data)
} else {
cat("LinkBike network not found in the API data.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
library(httr)
library(jsonlite)
library(dplyr)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Initialize an empty data frame to store the results
citybike_info <- data.frame()
# List of network names you are interested in and their corresponding cities
network_cities <- c(
"HELLO CYCLING Tokyo" = "Tokyo",
"Vélib' Métropole" = "Paris",
"Gira" = "Lisbon",
"Citi Bike" = "New York City",
"BikeSampa" = "São Paulo",
"Dubai Careem BIKE" = "Dubai",
"Bicing" = "Barcelona",
"Seoul Bike 따릉이" = "Seoul",
"Tembici" = "Bogotá",
"Beryl - Greater Manchester" = "Manchester"
)
# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)
# Check if the response is in JSON format
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
# Make sure the networks_data is a list and contains the 'networks' key
if ("networks" %in% names(networks_data)) {
networks_list <- networks_data$networks
# Loop through each network and fetch its detailed information if it's in the desired list
for (network in networks_list) {
network_name <- network$name
# Check if the network is in the list of desired networks
if (network_name %in% names(network_cities)) {
network_id <- network$id
city_name <- network_cities[network_name]  # Get the corresponding city name
# Construct the URL for the specific network's detailed information
network_url <- paste0(base_url, "/", network_id)
# Fetch detailed information about the network
network_response <- GET(network_url)
network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
# Extract station details and add new columns for the network name and city
stations <- network_details$network$stations
stations$network_name <- network_name
stations$city_name <- city_name  # Add the city name as a new column
# Ensure that the 'extra$uid' column is of the same type across all networks
if("extra" %in% names(stations)) {
stations$extra$uid <- as.character(stations$extra$uid)
}
# Combine the new stations data with the citybike_info dataset
citybike_info <- bind_rows(citybike_info, stations)
}
}
# Print the combined citybike_info data with the new city_name column
print(citybike_info)
} else {
cat("Error: 'networks' not found in the API response.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
# Load required libraries
library(httr)
library(jsonlite)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Fetch all networks to find LinkBike
response <- GET(base_url)
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
# Filter for LinkBike network
linkbike_network <- networks_data$networks[networks_data$networks$name == "Shymkentbike",]
if (nrow(linkbike_network) > 0) {
linkbike_id <- linkbike_network$id
linkbike_url <- paste0(base_url, "/", linkbike_id)
# Fetch detailed information about LinkBike network
linkbike_response <- GET(linkbike_url)
linkbike_data <- content(linkbike_response, as = "parsed", simplifyDataFrame = TRUE)
# Print detailed information
print(linkbike_data)
} else {
cat("LinkBike network not found in the API data.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
# Load required libraries
library(httr)
library(jsonlite)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Fetch all networks to find LinkBike
response <- GET(base_url)
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = TRUE)
# Filter for LinkBike network
linkbike_network <- networks_data$networks[networks_data$networks$name == "SiXT",]
if (nrow(linkbike_network) > 0) {
linkbike_id <- linkbike_network$id
linkbike_url <- paste0(base_url, "/", linkbike_id)
# Fetch detailed information about LinkBike network
linkbike_response <- GET(linkbike_url)
linkbike_data <- content(linkbike_response, as = "parsed", simplifyDataFrame = TRUE)
# Print detailed information
print(linkbike_data)
} else {
cat("LinkBike network not found in the API data.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
```{r}
library(httr)
library(jsonlite)
library(dplyr)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Initialize an empty data frame to store the results
citybike_info <- data.frame()
# List of network names you are interested in and their corresponding cities
network_cities <- c(
"Xi'an Public Bicycle" = "Xi'An",
"Medina Careem BIKE" = "Medina",
"LinkBike" = "Penang",
"ThessBike" = "Thessaloniki",
"BisPArks BCycle" = "North Dakota",
"Accès Vélo" = "Saguenay",
"Slovnaft BAjk" = "Bratislava",
"SiXT" = "Riga",
"Skrova Bysykkel" = "Skrova",
"Shymkentbike" = "Shymkent"
)
# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)
# Check if the response is in JSON format
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
# Make sure the networks_data is a list and contains the 'networks' key
if ("networks" %in% names(networks_data)) {
networks_list <- networks_data$networks
# Loop through each network and fetch its detailed information if it's in the desired list
for (network in networks_list) {
network_name <- network$name
# Check if the network is in the list of desired networks
if (network_name %in% names(network_cities)) {
network_id <- network$id
city_name <- network_cities[network_name]  # Get the corresponding city name
# Construct the URL for the specific network's detailed information
network_url <- paste0(base_url, "/", network_id)
# Fetch detailed information about the network
network_response <- GET(network_url)
network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
# Extract station details and add new columns for the network name and city
stations <- network_details$network$stations
stations$network_name <- network_name
stations$city_name <- city_name  # Add the city name as a new column
# Ensure that the 'extra$uid' column is of the same type across all networks
if("extra" %in% names(stations)) {
stations$extra$uid <- as.character(stations$extra$uid)
}
# Combine the new stations data with the citybike_info dataset
citybike_info <- bind_rows(citybike_info, stations)
}
}
# Print the combined citybike_info data with the new city_name column
print(citybike_info)
} else {
cat("Error: 'networks' not found in the API response.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
library(httr)
library(jsonlite)
library(dplyr)
# Base API URL
base_url <- "http://api.citybik.es/v2/networks"
# Initialize an empty data frame to store the results
citybike_info <- data.frame()
# List of network names you are interested in and their corresponding cities
network_cities <- c(
"HELLO CYCLING Tokyo" = "Tokyo",
"Vélib' Métropole" = "Paris",
"Gira" = "Lisbon",
"Citi Bike" = "New York City",
"BikeSampa" = "São Paulo",
"Dubai Careem BIKE" = "Dubai",
"Bicing" = "Barcelona",
"Seoul Bike 따릉이" = "Seoul",
"Tembici" = "Bogotá",
"Beryl - Greater Manchester" = "Manchester",
"Xi'an Public Bicycle" = "Xi'An",
"Medina Careem BIKE" = "Medina",
"LinkBike" = "Penang",
"ThessBike" = "Thessaloniki",
"BisPArks BCycle" = "North Dakota",
"Accès Vélo" = "Saguenay",
"Slovnaft BAjk" = "Bratislava",
"SiXT" = "Riga",
"Skrova Bysykkel" = "Skrova",
"Shymkentbike" = "Shymkent"
)
# Fetch all networks to get a list of all available networks globally
response <- GET(base_url)
# Check if the response is in JSON format
if (http_type(response) == "application/json") {
networks_data <- content(response, as = "parsed", simplifyDataFrame = FALSE)
# Make sure the networks_data is a list and contains the 'networks' key
if ("networks" %in% names(networks_data)) {
networks_list <- networks_data$networks
# Loop through each network and fetch its detailed information if it's in the desired list
for (network in networks_list) {
network_name <- network$name
# Check if the network is in the list of desired networks
if (network_name %in% names(network_cities)) {
network_id <- network$id
city_name <- network_cities[network_name]  # Get the corresponding city name
# Construct the URL for the specific network's detailed information
network_url <- paste0(base_url, "/", network_id)
# Fetch detailed information about the network
network_response <- GET(network_url)
network_details <- content(network_response, as = "parsed", simplifyDataFrame = TRUE)
# Extract station details and add new columns for the network name and city
stations <- network_details$network$stations
stations$network_name <- network_name
stations$city_name <- city_name  # Add the city name as a new column
# Ensure that the 'extra$uid' column is of the same type across all networks
if("extra" %in% names(stations)) {
stations$extra$uid <- as.character(stations$extra$uid)
}
# Combine the new stations data with the citybike_info dataset
citybike_info <- bind_rows(citybike_info, stations)
}
}
# Print the combined citybike_info data with the new city_name column
print(citybike_info)
} else {
cat("Error: 'networks' not found in the API response.\n")
}
} else {
cat("Failed to retrieve data from the CityBikes API.\n")
}
View(citybike_info)
# Filter the dataset for stations in Tokyo
tokyo_stations <- citybike_info %>%
filter(city_name == "Tokyo")
# Print the filtered dataset
print(tokyo_stations)
library(rvest)
library(tibble)
# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo")
# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"
# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())
# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"
# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
# Construct the full URL for each city
url <- paste0(base_url, city)
# Read the HTML content of the webpage
webpage <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading ", city, " page: ", e$message)
return(NULL)
})
if (!is.null(webpage)) {
# Extract the population density value using the provided XPath
pop_density <- webpage %>%
html_nodes(xpath = pop_density_xpath) %>%
html_text() %>%
trimws()  # Remove any leading or trailing whitespace
# If the population density is not found, set it as NA
pop_density <- ifelse(length(pop_density) == 0 || pop_density == "", NA, pop_density)
# Add the city and its population density to the tibble
city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}
}
library(rvest)
library(tibble)
# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo")
# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"
# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())
# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"
# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
# Construct the full URL for each city
url <- paste0(base_url, city)
# Read the HTML content of the webpage
webpage <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading ", city, " page: ", e$message)
return(NULL)
})
if (!is.null(webpage)) {
# Extract the population density value using the provided XPath
pop_density <- webpage %>%
html_nodes(xpath = pop_density_xpath) %>%
html_text() %>%
trimws()  # Remove any leading or trailing whitespace
# If the population density is not found, set it as NA
if (length(pop_density) > 0) {
pop_density <- pop_density[1]  # Take the first value if there are multiple
} else {
pop_density <- NA
}
# Add the city and its population density to the tibble
city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}
}
# Print the final tibble
print(city_data)
library(rvest)
library(tibble)
# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Seoul")
# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"
# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())
# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"
# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
# Construct the full URL for each city
url <- paste0(base_url, city)
# Read the HTML content of the webpage
webpage <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading ", city, " page: ", e$message)
return(NULL)
})
if (!is.null(webpage)) {
# Extract the population density value using the provided XPath
pop_density <- webpage %>%
html_nodes(xpath = pop_density_xpath) %>%
html_text() %>%
trimws()  # Remove any leading or trailing whitespace
# If the population density is not found, set it as NA
if (length(pop_density) > 0) {
pop_density <- pop_density[1]  # Take the first value if there are multiple
} else {
pop_density <- NA
}
# Add the city and its population density to the tibble
city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}
}
# Print the final tibble
print(city_data)
library(rvest)
library(tibble)
# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Paris", "Lisbon", "New York City", "São Paulo", "Dubai", "Barcelona", "Seoul",
"Bogotá", "Manchester", "Xi'An", "Medina", "Penang", "Thessaloniki", "North Dakota",
"Saguenay", "Bratislava", "Riga", "Skrova", "Shymkent")
# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"
# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())
# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"
# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
# Construct the full URL for each city
url <- paste0(base_url, city)
# Read the HTML content of the webpage
webpage <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading ", city, " page: ", e$message)
return(NULL)
})
if (!is.null(webpage)) {
# Extract the population density value using the provided XPath
pop_density <- webpage %>%
html_nodes(xpath = pop_density_xpath) %>%
html_text() %>%
trimws()  # Remove any leading or trailing whitespace
# If the population density is not found, set it as NA
if (length(pop_density) > 0) {
pop_density <- pop_density[1]  # Take the first value if there are multiple
} else {
pop_density <- NA
}
# Add the city and its population density to the tibble
city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}
}
# Print the final tibble
print(city_data)
library(rvest)
library(tibble)
# Define the list of city names (you only need to modify this line to add more cities)
cities <- c("Tokyo", "Paris", "Lisbon", "New_York_City", "São_Paulo", "Dubai", "Barcelona", "Seoul",
"Bogotá", "Manchester", "Xi%27an", "Medina", "Penang", "Thessaloniki", "North_Dakota",
"Saguenay", "Bratislava", "Riga", "Skrova", "Shymkent")
# Base URL for the Wikipedia pages
base_url <- "https://en.wikipedia.org/wiki/"
# Initialize an empty tibble to store results
city_data <- tibble(City = character(), Population_Density = character())
# Define the XPath for population density
pop_density_xpath <- "//*[@id='mw-content-text']/div[1]/table[1]/tbody/tr[30]/td/text()"
# Loop through each city name, construct the URL, and extract the population density
for (city in cities) {
# Construct the full URL for each city
url <- paste0(base_url, city)
# Read the HTML content of the webpage
webpage <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading ", city, " page: ", e$message)
return(NULL)
})
if (!is.null(webpage)) {
# Extract the population density value using the provided XPath
pop_density <- webpage %>%
html_nodes(xpath = pop_density_xpath) %>%
html_text() %>%
trimws()  # Remove any leading or trailing whitespace
# If the population density is not found, set it as NA
if (length(pop_density) > 0) {
pop_density <- pop_density[1]  # Take the first value if there are multiple
} else {
pop_density <- NA
}
# Add the city and its population density to the tibble
city_data <- bind_rows(city_data, tibble(City = city, Population_Density = pop_density))
}
}
# Print the final tibble
print(city_data)
